{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Direct Environment and Policy Search on the MSD\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate the MSD environment and some wrappers.\n",
    "The first wrapper one-hot-encodes of the actions, the second normalizes the state,\n",
    "and the last adds a time dependency to the state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from system.MSD.MSDSystem import MSDSystem\n",
    "from system.Wrappers.OneHotEncodingWrapper import OHEWrapper\n",
    "from system.Wrappers.StateScaleWrapper import StateScaleWrapper\n",
    "from system.Wrappers.StationarySystemWrapper import StationarySystemWrapper\n",
    "\n",
    "sys = MSDSystem(horizon=100,\n",
    "                equilibrium=0.2,\n",
    "                actions_value=[-0.3, -0.1, 0.0, 0.1, 0.3],\n",
    "                target_parameters_reward=[0.5, -0.3, 0.1],\n",
    "                cost_omega_zeta=[0.5, 0.5],\n",
    "                accuracy=1.0,\n",
    "                actions_discretization=0.05,\n",
    "                position_interval=[0.198, 0.202],\n",
    "                speed_interval=[-0.01, 0.01],\n",
    "                feasible_set={\"omega_interval\": [0.1, 1.5],\n",
    "                              \"zeta_interval\": [0.1, 1.5],\n",
    "                              \"phi_interval\": [-2.0, 2.0]})\n",
    "\n",
    "sys = OHEWrapper(sys)\n",
    "sys = StateScaleWrapper(sys,\n",
    "                        loc=[0.2, 0],\n",
    "                        scale=[0.005, 0.02])\n",
    "sys = StationarySystemWrapper(sys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate a deterministic agent depending on two models (parametric functions).\n",
    "The first model stands for the system parameters and the second is a parametric control policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from agent.trainable.DESGA.DSSAAgent import DSSAAgent\n",
    "from model.investment.DeterministicParameterModel import DeterministicParameterModel\n",
    "from model.decision.CategoricalDistModel import CategoricalDistModel\n",
    "\n",
    "agent = DSSAAgent(InvestmentModel=DeterministicParameterModel,\n",
    "                  OperationModel=CategoricalDistModel).initialize(env=sys,\n",
    "                                                                  investment_pol=dict(),\n",
    "                                                                  operation_pol={\"layers\": (64,)})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate a log for the learning curves."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import get_version\n",
    "from algo.joint.LoggerDESGA import LoggerDESGA\n",
    "\n",
    "logdir = \"./experiments\"\n",
    "model_name = \"msd-env\"\n",
    "v = get_version(model_name, logdir)\n",
    "log_path = os.path.join(logdir, f\"{model_name}-v{v}\")\n",
    "logger = LoggerDESGA(log_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate an algorithm for learning the parameters of the agent's models for maximizing the return in the environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from algo.joint.DESGA import ReinforceDESGA\n",
    "\n",
    "algo = ReinforceDESGA(env=sys, agent=agent)\n",
    "algo.initialize(nb_iterations=500,\n",
    "                optimizer_parameters={\"lr\": 0.005},\n",
    "                batch_size=64,\n",
    "                mc_samples=64,\n",
    "                system_fit=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit the agent with the algorithm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "algo.fit(logger)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simulate the policy in the environment and print the expected return."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative reward :  99.9478530883789\n"
     ]
    }
   ],
   "source": [
    "from runner.TrajectoriesSampler import TrajectoriesSampler\n",
    "\n",
    "sampler = TrajectoriesSampler(sys, agent)\n",
    "_, _, reward_batch, _, _ = sampler.sample(100)\n",
    "print(\"Average cumulative reward : \", sampler.cumulative_reward(reward_batch))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}